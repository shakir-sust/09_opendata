---
title: "Daymet"
format: html
---

# Introduction  

Open-source data bases are an important source of data, especially for larger-scale (both in time and space) projects that have some field-level data collected along with their spatial coordinate.  

These open-source data bases may be related to crop, soils, weather, remote sensing, elevation, and many other potential variables.  

These data can be pulled and used to create predictor variables in a machine learning workflow.  

Today, we are going to explore how to explore a field-collected data set, and how to complement it by downloading and pre-processing an open-source data base for weather data.  

# Learning objectives  

Our learning objectives are to:  
  - Learn about and implement the download and pre-processing of a spatial and temporal weather open-source data base  
  - Pull weather data for multiple sites and years  
  - Export it to file for future reuse  

# Setup  

Important Note: CRAN is like the app store for R packages. Everytime we use "install.packages()", we use CRAN as the source of that package. However, some people may not want to submit a package in CRAN, instead make it available through GitHub. If we want to install a package through GitHub, we cannot use "install.packages()" because "install.packages()" feeds from CRAN (CRAN is the official R package app store). In order to install packages from GitHub, we use "remotes::install_github()" [and not "install.packages()"].

```{r}
#| message: false
#| warning: false

#install.packages("sf") #to manipulate vector geospatial files
#install.packages("daymetr") #to retrieve data from daymet database website through R
#install.packages("remotes") #to install R packages that are not available on CRAN, and available on GitHub
#remotes::install_github("ropensci/USAboundaries") 
#remotes::install_github("ropensci/USAboundariesData")

#Note: CRAN is like the app store for R packages. Everytime we use "install.packages()", we use CRAN as the source of that package. However, some people may not want to submit a package in CRAN, instead make it available through GitHub. If we want to install a package through GitHub, we cannot use "install.packages()" because "install.packages()" feeds from CRAN (CRAN is the official R package app store). In order to install packages from GitHub, we use "remotes::install_github()" [and not "install.packages()"].


library(tidyverse) #need to load "tidyverse" package at first

library(sf) # for US map #to manipulate vector geo-spatial points
library(daymetr) #to retrieve data from daymet database website through R
library(remotes)
library(USAboundaries) # for US state boundaries
library(USAboundariesData)

```


```{r}

field <- read_csv("../data/cotton_fielddata.csv")

field

#read_csv() = comes from one of the tidyverse packages; returns data as tibble
#read.csv() = Base R function; returns a dataframe

#lat = latitude
#lon = longitude
#strength_gtex = strength of the fiber per gram tex (quality parameter of fiber)


```

In the output, latitude and longitude are the 2 geospatial columns that we need to place anything in space. 

# Study description  

This data set comprises a study conducted across the Cotton Belt producing region of the US (from GA to CA) from 1980 to 2020.

The goal of the study was to measure cotton fiber yield and quality in different sites and over time.  

The data contains the following columns:  

  - **year**: the year of the study  
  - **site**: the location of the study  
  - **lat**: the latitude (in degrees) of the location  
  - **lon**: the longitude (in degrees) of the location  
  - **strength_gtex**: cotton fiber strength (in g/tex)

# EDA  

```{r}
summary(field)
```
*For USA:* latitude would have to be positive numbers because we are above the equator, and longitude would have to be negative number because we are to the west of the Greenwich line. So, always look at the summary of the geospatial data (i.e., latitude and longitude) to make sure that the current data that we are working on makes sense.  

How many unique years?

```{r unique years}

unique(field$year) %>% #gives a list
  length() #gives the exact number of unique years

```

41 distinct years in the data set.  

Now, how many unique sites?  

```{r unique sites}

unique(field$site) %>% #gives a list
  length() #gives the exact number of unique sites

#To widen our R knowledge, we will use a different function now 
field %>%
  distinct(site) %>% #Returns a tibble because it is a tidyverse package
  nrow() #returns number of rows

```

65 unique sites.  

So, 65 unique sites across 48 unique years for a total of 698  (= 65 x 48) site year combinations in this dataset.

What is the statistical distribution of fiber strength?  

Note: We use "geom_density()" to obtain distribution of a given variable in the form of density plot.
geom_density() gives us the outline of the distribution, but don't indicate where the points exist.
In order to get that, we can use geom_rug() which adds a vertical bar at the bottom of the density plot created by geom_density() to indicate where each observation has happened.

```{r fiber strength}

#we use "geom_density()" to obtain distribution of a given variable in the form of density plot

ggplot(data = field) +
  geom_density(aes(x = strength_gtex)) + #to get a density plot for "strength_gtex" [here we don't need a y variable because y is calculated by "geom_density()"]
  geom_rug(aes(x = strength_gtex)) #adds a vertical bar at the bottom of the geom_density() to where each observation has happened.

```
Looking at the density plot, the mean is around 30 [from the summary output, "Mean: 30.37" for "strength_gtex"].

Creating a map of USA and adding points to understand the spatial distribution of the dataset we are using:

```{r create map of USA and add points}

#select() works on columns
#filter() works on rows

states <- us_states() %>% # "us_states()" function comes from "USboundaries" package: returns a "simple feature" or "sf" object, which is a geospatial object in R
  filter( !(state_abbr %in% c("PR", "AK", "HI")) ) #to remove "PR" (Puerto Rico), "AK" (Alaska), and "HI" (Hawaii) i.e., to keep all states that are NOT "PR", "AK", and "HI" #we use "%in%" to filter more than one entry 
  
ggplot() +
  geom_sf(data = states) + #"geom_sf()" is used to plot "sf" object, which we just created above as "states" object; plots all states and territories of USA
  geom_point(data = field,
             aes(x = lon,
                 y = lat)
             )

```

# Open weather data - Daymet  
**Daymet** is an open-source weather data base developed by NASA (https://daymet.ornl.gov).  

> Daymet provides long-term, continuous, gridded estimates of daily weather and climatology variables by interpolating and extrapolating ground-based observations through statistical modeling techniques.  

Data characteristics:    
  - Spatial extent: North America (US, CAN, MEX)  
  - Spatial resolution: **1 km**  
  - Temporal resolution: **daily**  
  - Temporal extent: **1980 to present day**  
  - Variables included:  
    - day length (secs/day)    
    - precipitation (mm/day)  
    - shortwave radiation (W/m2)  
    - snow water equivalent (kg/m2)  
    - maximum air temperature (C)  
    - minimum air temperature (C)  
    - water vapor pressure (Pa)  

Daymet provides an application programing interface (API) for users to make queries and download data.  

In R, we'll use a package called `daymetr` that facilitates makeing queries to Daymet API.  

The main function we'll use is called `download_daymet()`. Let's check its documentation.  
```{r}

help("download_daymet")

```

From the documentation, we see we need to provide:  
  - latitude (we have it)  
  - longitude (we have it)  
  - start and end of year to download (we have it)  
  
Let's try it with the first site-year on the data frame.  

# Daymet - one site-year  
```{r one site-year}
field

daymet_one <- download_daymet(site = field$site[[1]],
                              lat = field$lat[[1]],
                              lon = field$lon[[1]],
                              start = field$year[[1]],
                              end = field$year[[1]],
                              simplify = T #to get tidyverse friendly data
                              )

daymet_one

#2,555 rows because the 1st 365 days are about day length, then it resets the 2nd set of 365 days for precipitation, then it again resets the next set of 365 days for all the weather variables that are under the "measurement" column

#To get separate columns for all weather variables for 365 days, we will pivot_wider() [next code chunk]

```

How many rows above? Why?  

```{r}

#To get separate columns for all weather variables for 365 days, we will pivot_wider() 

daymet_one %>%
  pivot_wider(names_from = measurement,
              values_from = value
              )

```
How many rows above? Why?  

Answer: 365 rows because we did pivot_wider() for the "measurement" column for 365 days, which created separate columns for all weather variablws for these 365 days

We just pulled daily weather data for one site and one year. Great!  

Now we just need to do that again for the remaining **697** site-years!  



# Daymet - all site-years  
For that, let's use the **map()** family of functions from the purrr package.  

**WARNING**: the chunk below took about **3 minutes** to run on my laptop.  

```{r}

#Iteration workflow creation (very very important)

daymet_all <- field %>%
  mutate(weather = pmap(list(.y = year, 
                             .site = site,
                             .lat = lat,
                             .lon = lon),
                        function(.y, .site, .lat, .lon)
                          download_daymet(
                            site = .site,
                            lat = .lat,
                            lon = .lon,
                            start = .y,
                            end = .y,
                            simplify = T,
                            silent = T) %>%
                          rename(.year = year,
                                 .site = site
                                 )
                        )) #using pmap() to iterate over all 697 site-years

head(daymet_all) #To look at the 1st 6 rows

```

Let's inspect weather data for the first site-year.  

```{r}

daymet_all$weather[[1]]

```

Now let's unnest the weather column. 

```{r}

daymet_all_unnest <- daymet_all %>%
  unnest(weather) %>% #To unnest the data frames inside all cells of weather column into the main data frame
  pivot_wider(names_from = measurement,
              values_from = value) %>% #we want to change the "measurement" column from long form to wide form
  janitor::clean_names()

daymet_all_unnest

#254,770 rows = 698 site years * 365 days (in a year) 

```

How many rows? Why?  

Answer: 254,770 rows = 698 site years * 365 days (in a year) 

# Exporting  
We don't want to have to make an API call every time we'll work with this weather data.  

Therefore, a best practice here is to pull weather data once and export it to file so we can reuse it any time without having to download again.  

Let's do that below:  
```{r}

write_csv(daymet_all_unnest,
          "../data/fieldweatherdata.csv"
          )

```


# Summary  
In this exercise, we:  
  - Used year, longitude, and latitude to pull daily weather data from Daymet  
  - Wrote code to automate and iterate this process for 698 site-years  
  - Exported the data for future reuse  

# Other open-source data APIs in R  
There are MANY open-source data APIs relevant for agriculture applications, most of which have an R implementation.  

Some of the ones I used in the past:  
  - **USDA NASS crop statistics**: https://cran.r-project.org/web/packages/rnassqs/vignettes/rnassqs.html  
  - Soil properties:  
    - **POLARIS**: https://github.com/lhmrosso/XPolaris  [USA level]
    - **Soilsgrid**: https://rpubs.com/ials2un/soilgrids_webdav  [Whole world level]
    - **SSURGO**: https://search.r-project.org/CRAN/refmans/FedData/html/get_ssurgo.html  [SSURGO: USDA Web Soil Survey Online]    
  - **Soil water**: https://leombastos.github.io/bastoslab/teaching/2023-aghack-vwc/2023-aghack-vwc.html     
  - **Drought monitor**: https://droughtmonitor.unl.edu/DmData/DataDownload/WebServiceInfo.aspx    
  - **Remote sensing**: https://github.com/bevingtona/planetR  
  - **Elevation**: https://github.com/jhollist/elevatr  
  





