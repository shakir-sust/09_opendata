---
title: "Daymet"
format: html
---

# Introduction  

Open-source data bases are an important source of data, especially for larger-scale (both in time and space) projects that have some field-level data collected along with their spatial coordinate.  

These open-source data bases may be related to crop, soils, weather, remote sensing, elevation, and many other potential variables.  

These data can be pulled and used to create predictor variables in a machine learning workflow.  

Today, we are going to explore how to explore a field-collected data set, and how to complement it by downloading and pre-processing an open-source data base for weather data.  

# Learning objectives  

Our learning objectives are to:  
  - Learn about and implement the download and pre-processing of a spatial and temporal weather open-source data base  
  - Pull weather data for multiple sites and years  
  - Export it to file for future reuse  

# Setup  

Important Note: CRAN is like the app store for R packages. Everytime we use "install.packages()", we use CRAN as the source of that package. However, some people may not want to submit a package in CRAN, instead make it available through GitHub. If we want to install a package through GitHub, we cannot use "install.packages()" because "install.packages()" feeds from CRAN (CRAN is the official R package app store). In order to install packages from GitHub, we use "remotes::install_github()" [and not "install.packages()"].

```{r}
#| message: false
#| warning: false

#install.packages("sf") #to manipulate vector geospatial files
#install.packages("daymetr") #to retrieve data from daymet database website through R
#install.packages("remotes") #to install R packages that are not available on CRAN, and available on GitHub
#remotes::install_github("ropensci/USAboundaries") 
#remotes::install_github("ropensci/USAboundariesData")

#Note: CRAN is like the app store for R packages. Everytime we use "install.packages()", we use CRAN as the source of that package. However, some people may not want to submit a package in CRAN, instead make it available through GitHub. If we want to install a package through GitHub, we cannot use "install.packages()" because "install.packages()" feeds from CRAN (CRAN is the official R package app store). In order to install packages from GitHub, we use "remotes::install_github()" [and not "install.packages()"].


library(tidyverse) #need to load "tidyverse" package at first

library(sf) # for US map #to manipulate vector geo-spatial points
library(daymetr) #to retrieve data from daymet database website through R
library(remotes)
library(USAboundaries) # for US state boundaries
library(USAboundariesData)

```


```{r}

field <- read_csv("../data/cotton_fielddata.csv")

field

#read_csv() = comes from one of the tidyverse packages; returns data as tibble
#read.csv() = Base R function; returns a dataframe

#lat = latitude
#lon = longitude
#strength_gtex = strength of the fiber per gram tex (quality parameter of fiber)


```

In the output, latitude and longitude are the 2 geospatial columns that we need to place anything in space. 

# Study description  

This data set comprises a study conducted across the Cotton Belt producing region of the US (from GA to CA) from 1980 to 2020.

The goal of the study was to measure cotton fiber yield and quality in different sites and over time.  

The data contains the following columns:  

  - **year**: the year of the study  
  - **site**: the location of the study  
  - **lat**: the latitude (in degrees) of the location  
  - **lon**: the longitude (in degrees) of the location  
  - **strength_gtex**: cotton fiber strength (in g/tex)

# EDA  

```{r}
summary(field)
```
*For USA:* latitude would have to be positive numbers because we are above the equator, and longitude would have to be negative number because we are to the west of the Greenwich line. So, always look at the summary of the geospatial data (i.e., latitude and longitude) to make sure that the current data that we are working on makes sense.  

How many unique years?

```{r unique years}

unique(field$year) %>% #gives a list
  length() #gives the exact number of unique years

```

41 distinct years in the data set.  

Now, how many unique sites?  

```{r unique sites}

unique(field$site) %>% #gives a list
  length() #gives the exact number of unique sites

#To widen our R knowledge, we will use a different function now 
field %>%
  distinct(site) %>% #Returns a tibble because it is a tidyverse package
  nrow() #returns number of rows

```

65 unique sites.  

So, 65 unique sites across 48 unique years for a total of 698  (= 65 x 48) site year combinations in this dataset.

What is the statistical distribution of fiber strength?  

Note: We use "geom_density()" to obtain distribution of a given variable in the form of density plot.
geom_density() gives us the outline of the distribution, but don't indicate where the points exist.
In order to get that, we can use geom_rug() which adds a vertical bar at the bottom of the density plot created by geom_density() to indicate where each observation has happened.

```{r fiber strength}

#we use "geom_density()" to obtain distribution of a given variable in the form of density plot

ggplot(data = field) +
  geom_density(aes(x = strength_gtex)) + #to get a density plot for "strength_gtex" [here we don't need a y variable because y is calculated by "geom_density()"]
  geom_rug(aes(x = strength_gtex)) #adds a vertical bar at the bottom of the geom_density() to where each observation has happened.

```
Looking at the density plot, the mean is around 30 [from the summary output, "Mean: 30.37" for "strength_gtex"].

Creating a map of USA and adding points to understand the spatial distribution of the dataset we are using:

```{r create map of USA and add points}

#select() works on columns
#filter() works on rows

states <- us_states() %>% # "us_states()" function comes from "USboundaries" package: returns a "simple feature" or "sf" object, which is a geospatial object in R
  filter( !(state_abbr %in% c("PR", "AK", "HI")) ) #to remove "PR" (Puerto Rico), "AK" (Alaska), and "HI" (Hawaii) from the rows of "states" object i.e., to keep all states that are NOT "PR", "AK", and "HI" #we use "%in%" to filter (i.e., work on) more than one state/entry #if we wanted to filter just 1 state, we would use == sign e.g., !(state_abbr == c("PR"))
  
ggplot() +
  geom_sf(data = states) + #"geom_sf()" is used to plot "sf" object, which we just created above as "states" object; plots all states and territories of USA
  geom_point(data = field,
             aes(x = lon, #"Longitude" goes on longitude
                 y = lat) #"Latitude" goes on latitude
             )

```

# Open weather data - Daymet  

**Daymet** is an open-source weather data base developed by NASA (https://daymet.ornl.gov).  

> Daymet provides long-term, continuous, gridded estimates of daily weather and climatology variables by interpolating and extrapolating ground-based observations through statistical modeling techniques.  

Data characteristics:    
  - Spatial extent: North America (US, CAN, MEX)  
  - Spatial resolution: **1 km**  
  - Temporal resolution: **daily**  
  - Temporal extent: **1980 to present day**  
  - Variables included:  
    - day length (secs/day)    
    - precipitation (mm/day)  
    - shortwave radiation (W/m2)  
    - snow water equivalent (kg/m2)  
    - maximum air temperature (C)  
    - minimum air temperature (C)  
    - water vapor pressure (Pa)  

Daymet provides an "Application Programming Interface" (API) for users to make queries and download data.  

In R, we'll use a package called `daymetr` that facilitates makeing queries to Daymet API.  

The main function we'll use is called `download_daymet()`. Let's check its documentation.  
```{r}

help("download_daymet")

```

From the documentation, we see we need to provide the following to parse weather data from daymet:  
  - latitude (we have it)  
  - longitude (we have it)  
  - start and end of year to download (we have it)  
  
Let's try it with the first site-year on the data frame.  

# Daymet - one site-year  [downloading weather data from "Daymet" for only 1 site and 1 year]  

```{r one site-year}

field

daymet_one <- download_daymet(site = field$site[[1]], #we used [[1]] to get weather data just for the 1st site
                              lat = field$lat[[1]],
                              lon = field$lon[[1]],
                              start = field$year[[1]],
                              end = field$year[[1]], #we specify the same year that we specifed in "start"
                              simplify = T #to get tidyverse friendly data
                              )

daymet_one #"daymet_one" is the object we have used to store weather data for only 1 site and 1 year

#2,555 rows because the 1st 365 days are about day length, then it resets the 2nd set of 365 days for precipitation, then it again resets the next set of 365 days for all the weather variables that are under the "measurement" column

#To get separate columns for all weather variables for 365 days, we will pivot_wider() [next code chunk]

```

How many rows above? Why?  

```{r}

#To get separate columns for all weather variables for 365 days, we will pivot_wider() 

daymet_one %>%
  pivot_wider(names_from = measurement,
              values_from = value
              )

```
How many rows above? Why?  

Answer: 365 rows because we did pivot_wider() for the "measurement" column for 365 days, which created separate columns for all weather variables for these 365 days

We just pulled daily weather data for one site and one year. Great!  

Now we just need to do that again for the remaining **697** site-years!  

# Daymet - all site-years  

For that, let's use the **map()** family of functions from the purrr package.  

We will use `pmap` to download the weather data for all 698 site-years and save in "daymet_all" object (just in 1 code, using iteration).

**WARNING**: the chunk below took about **3 minutes** to run on my laptop.  

```{r}

#Iteration workflow creation (very very important)

daymet_all <- field %>% #we start with our dataframe which is "field"
  mutate(weather = pmap(list(.y = year, # .y is the place holder for "year" #after the = sign of .y, we must specify the actual name of the column [which is very important] # .y is what we decide to name the place holder; we can use anything to name this place holder(e.g., .year) , but we usually name these place holders as .[column_name] to avoid complexity down the road
                             .site = site, # .site is the place holder for "site"
                             .lat = lat, # .lat is the place holder for "lat"
                             .lon = lon), # .lon is the place holder for "lon" #notice that the comma " , " is after the 1st bracket " ) " to close the "list()" function
                        function(.y, .site, .lat, .lon) #Inside "function()", we specify the place holder names that we used inside the "list()" function
                          download_daymet( 
                            site = .site, #specifying ".site" placeholder for "site = " argument
                            lat = .lat, #specifying ".lat" placeholder for "lat = " argument
                            lon = .lon, #specifying ".lon" placeholder for "lon = " argument
                            start = .y, #specifying ".y" (placeholder for "year") for "start = " argument (for starting year)
                            end = .y, #specifying ".y" (placeholder for "year") for "start = " argument (for end year, which will always be the same as the start year) # we are giving the same year as "start" and "end" because that's how the function understands it
                            simplify = T,
                            silent = T) %>% #end of " download_daymet()" function
                          rename(.year = year,
                                 .site = site) #we always rename the "year" and "site" because if we don't rename year and site, daymet() will give us 2 conflicting columns [one "year" for our data, another "year" for daymet data; similarly, one "site" for our data, another "site" for "daymet" data], which we don't want #so, this is a very important wrangling step 
                        )) #We use "download_daymet()" to download weather data in batch from "Daymet" database #we use "pmap()" to iterate over all 697 site-years

head(daymet_all) #To look at the 1st 6 rows

```

The "weather" column in the output is a nested column (i.e., the "weather" column contains a dataframe e.g., tibble instead of just 1 single value)

Let's inspect weather data for the first site-year.  

```{r}

daymet_all$weather[[1]]

```

Now let's unnest the weather column. 

```{r}

daymet_all_unnest <- daymet_all %>%
  unnest(weather) %>% #To unnest the "weather" column i.e., to unnest the "tibble" dataframes inside all of the cells of the "weather" column and then to bring them back into the main level of the data frame
  pivot_wider(names_from = measurement, #to pivot wide the names from the "measurement" column
              values_from = value) %>% #to pivot wide the values from the "value" column #because we want to change the "measurement" column from long form to wide form
  janitor::clean_names() #to clean and standardize the column names with all lower cases and underscores to fill within words 

daymet_all_unnest

#254,770 rows = 698 site years * 365 days (in a year) 

```

How many rows? Why?  

Answer: 254,770 rows = 698 site years * 365 days (in a year) 

# Exporting  

[Note: Every time we pull open source data, it is highly recommended to export the data as a .csv file]

We don't want to have to make an API call every time we'll work with this weather data.  

Therefore, a best practice here is to pull weather data once and export it to file so we can reuse it any time without having to download again.  

Let's do that below:  
```{r}

write_csv(daymet_all_unnest,
          "../data/fieldweatherdata.csv"
          )

```


# Summary  
In this exercise, we:  
  - Used year, longitude, and latitude to pull daily weather data from Daymet  
  - Wrote code to automate and iterate this process for 698 site-years  
  - Exported the data for future reuse  

# Other open-source data APIs in R  
There are MANY open-source data APIs relevant for agriculture applications, most of which have an R implementation.  

Some of the ones I used in the past:  
  - **USDA NASS (= National Agricultural Statistical Service) crop statistics**: https://cran.r-project.org/web/packages/rnassqs/vignettes/rnassqs.html  
  - Soil properties:  
    - **POLARIS**: https://github.com/lhmrosso/XPolaris  [USA level]
    - **Soilsgrid**: https://rpubs.com/ials2un/soilgrids_webdav  [Whole world level]
    - **SSURGO**: https://search.r-project.org/CRAN/refmans/FedData/html/get_ssurgo.html  [SSURGO: USDA Web Soil Survey Online]    
  - **Soil water**: https://leombastos.github.io/bastoslab/teaching/2023-aghack-vwc/2023-aghack-vwc.html     
  - **Drought monitor**: https://droughtmonitor.unl.edu/DmData/DataDownload/WebServiceInfo.aspx    
  - **Remote sensing**: https://github.com/bevingtona/planetR  [to pull remote sensing data from "Planet" satellite]
  - **Elevation**: https://github.com/jhollist/elevatr [to pull open-source elevation data for open fields to determine how to place blocks in a new farm]  
  





