---
title: "Daymet"
format: html
---

# Introduction  

Open-source data bases are an important source of data, especially for larger-scale (both in time and space) projects that have some field-level data collected along with their spatial coordinate.  

These open-source data bases may be related to crop, soils, weather, remote sensing, elevation, and many other potential variables.  

These data can be pulled and used to create predictor variables in a machine learning workflow.  

Today, we are going to explore how to explore a field-collected data set, and how to complement it by downloading and pre-processing an open-source data base for weather data.  

# Learning objectives  

Our learning objectives are to:  
  - Learn about and implement the download and pre-processing of a spatial and temporal weather open-source data base  
  - Pull weather data for multiple sites and years  
  - Export it to file for future reuse  

# Setup  

```{r}
#| message: false
#| warning: false

#install.packages("sf")
#install.packages("daymetr")
#install.packages("remotes") #To install R packages that are not available on CRAN, and available on GitHub
#remotes::install_github("ropensci/USAboundaries") #"remotes::install_github()"To install packages from GitHub
#remotes::install_github("ropensci/USAboundariesData")

library(tidyverse)
library(USAboundaries) # for US state boundaries
library(sf) # for US map #to manipulate vector geo-spatial points
library(daymetr)
```


```{r}

field <- read_csv("../data/cotton_fielddata.csv")

field

#read_csv() = comes from one of the tidyverse packages; returns data as tibble
#read.csv() = Base R function; returns a dataframe

#lat = latitude
#lon = longitude
#strength_gtex = strength of the fiber per gram tex (quality parameter of fiber)


```

# Study description  

This data set comprises a study conducted across the Cotton Belt producing region of the US (from GA to CA) from 1980 to 2020.

The goal of the study was to measure cotton fiber yield and quality in different sites and over time.  

The data contains the following columns:  

  - **year**: the year of the study  
  - **site**: the location of the study  
  - **lat**: the latitude (in degrees) of the location  
  - **lon**: the longitude (in degrees) of the location  
  - **strength_gtex**: cotton fiber strength (in g/tex)

# EDA  

```{r}
summary(field)
```
How many unique years?

```{r unique years}

unique(field$year) %>%
  length()

```

41 distinct years in the data set.  

Now, how many unique sites?  
```{r unique sites}

field %>%
  distinct(site) %>% #tidyverse package
  nrow() #returns number of rows

```

65 unique sites.  

What is the statistical distribution of fiber strength?  

```{r fiber strength}

ggplot(data = field) +
  geom_density(aes(x = strength_gtex)) +
  geom_rug(aes(x = strength_gtex))

```

```{r create map of USA and add points}

#select() works on columns
#filter() works on rows

states <- us_states() %>%
  filter( !(state_abbr %in% c("PR", "AK", "HI")) ) #we use "%in%" to filter more than one entry #to keep all states that are NOT "PR", "AK", and "HI"
  
ggplot() +
  geom_sf(data = states) +
  geom_point(data = field,
             aes(x = lon,
                 y = lat)
             )

```

# Open weather data - Daymet  
**Daymet** is an open-source weather data base developed by NASA (https://daymet.ornl.gov).  

> Daymet provides long-term, continuous, gridded estimates of daily weather and climatology variables by interpolating and extrapolating ground-based observations through statistical modeling techniques.  

Data characteristics:    
  - Spatial extent: North America (US, CAN, MEX)  
  - Spatial resolution: **1 km**  
  - Temporal resolution: **daily**  
  - Temporal extent: **1980 to present day**  
  - Variables included:  
    - day length (secs/day)    
    - precipitation (mm/day)  
    - shortwave radiation (W/m2)  
    - snow water equivalent (kg/m2)  
    - maximum air temperature (C)  
    - minimum air temperature (C)  
    - water vapor pressure (Pa)  

Daymet provides an application programing interface (API) for users to make queries and download data.  

In R, we'll use a package called `daymetr` that facilitates makeing queries to Daymet API.  

The main function we'll use is called `download_daymet()`. Let's check its documentation.  
```{r}

help("download_daymet")

```

From the documentation, we see we need to provide:  
  - latitude (we have it)  
  - longitude (we have it)  
  - start and end of year to download (we have it)  
  
Let's try it with the first site-year on the data frame.  

# Daymet - one site-year  
```{r one site-year}
field

daymet_one <- download_daymet(site = field$site[[1]],
                              lat = field$lat[[1]],
                              lon = field$lon[[1]],
                              start = field$year[[1]],
                              end = field$year[[1]],
                              simplify = T #to get tidyverse friendly data
                              )

daymet_one
```

How many rows above? Why?  

```{r}
daymet_one %>%
  pivot_wider(names_from = measurement,
              values_from = value
              )
```
How many rows above? Why?  

We just pulled daily weather data for one site and one year. Great!  

Now we just need to do that again for the remaining **697** site-years!  



# Daymet - all site-years  
For that, let's use the **map()** family of functions from the purrr package.  

**WARNING**: the chunk below took about **3 minutes** to run on my laptop.  

```{r}

#Iteration workflow creation (very very important)

daymet_all <- field %>%
  mutate(weather = pmap(list(.y = year,
                             .site = site,
                             .lat = lat,
                             .lon = lon),
                        function(.y, .site, .lat, .lon)
                          download_daymet(
                            site = .site,
                            lat = .lat,
                            lon = .lon,
                            start = .y,
                            end = .y,
                            simplify = T,
                            silent = T) %>%
                          rename(.year = year,
                                 .site = site
                                 )
                        ))

head(daymet_all)

```

Let's inspect weather data for the first site-year.  

```{r}

daymet_all$weather[[1]]

```

Now let's unnest the weather column. 

```{r}

daymet_all_unnest <- daymet_all %>%
  unnest(weather) %>% #To unnest the data frames inside each cells of weather column into the main data frame
  pivot_wider(names_from = measurement,
              values_from = value) %>%
  janitor::clean_names()

daymet_all_unnest

#698 site years * 365 days in a year = 254,770 rows

```

How many rows? Why?  

# Exporting  
We don't want to have to make an API call every time we'll work with this weather data.  

Therefore, a best practice here is to pull weather data once and export it to file so we can reuse it any time without having to download again.  

Let's do that below:  
```{r}

write_csv(daymet_all_unnest,
          "../data/fieldweatherdata.csv"
          )

```


# Summary  
In this exercise, we:  
  - Used year, longitude, and latitude to pull daily w
  eather data from Daymet  
  - Wrote code to automate and iterate this process for 698 site-years  
  - Exported the data for future reuse  

# Other open-source data APIs in R  
There are MANY open-source data APIs relevant for agriculture applications, most of which have an R implementation.  

Some of the ones I used in the past:  
  - **USDA NASS crop statistics**: https://cran.r-project.org/web/packages/rnassqs/vignettes/rnassqs.html  
  - Soil properties:  
    - **POLARIS**: https://github.com/lhmrosso/XPolaris  
    - **Soilsgrid**: https://rpubs.com/ials2un/soilgrids_webdav  
    - **SSURGO**: https://search.r-project.org/CRAN/refmans/FedData/html/get_ssurgo.html    
  - **Soil water**: https://leombastos.github.io/bastoslab/teaching/2023-aghack-vwc/2023-aghack-vwc.html     
  - **Drought monitor**: https://droughtmonitor.unl.edu/DmData/DataDownload/WebServiceInfo.aspx    
  - **Remote sensing**: https://github.com/bevingtona/planetR  
  - **Elevation**: https://github.com/jhollist/elevatr  
  





